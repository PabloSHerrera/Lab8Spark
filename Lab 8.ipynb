{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a1d8aa95-f592-45dd-956a-f30f9a6367d9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'dbutils' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 7\u001b[39m\n\u001b[32m      4\u001b[39m INPUT_XLSX = \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mBASE_DIR\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m/Bases de datos principales PNC.xlsx\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m      6\u001b[39m \u001b[38;5;66;03m# Crear la subcarpeta con API de Databricks (NO usar Path.mkdir/os.mkdir en /Volumes)\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m7\u001b[39m \u001b[43mdbutils\u001b[49m.fs.mkdirs(OUT_DIR)\n\u001b[32m      9\u001b[39m \u001b[38;5;66;03m# Verifica que todo exista\u001b[39;00m\n\u001b[32m     10\u001b[39m display(dbutils.fs.ls(BASE_DIR))\n",
      "\u001b[31mNameError\u001b[39m: name 'dbutils' is not defined"
     ]
    }
   ],
   "source": [
    "VOLUME_NAME = \"lab8\"  # <-- este es el que sí existe\n",
    "BASE_DIR = f\"/Volumes/workspace/default/{VOLUME_NAME}\"\n",
    "INPUT_XLSX = f\"{BASE_DIR}/Bases de datos principales PNC.xlsx\"\n",
    "\n",
    "# Crear la subcarpeta con API de Databricks (NO usar Path.mkdir/os.mkdir en /Volumes)\n",
    "dbutils.fs.mkdirs(OUT_DIR)\n",
    "\n",
    "# Verifica que todo exista\n",
    "display(dbutils.fs.ls(BASE_DIR))\n",
    "print(\"Excel path ->\", INPUT_XLSX)\n",
    "print(\"Output dir ->\", OUT_DIR)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "159e43d3-4d48-4295-8e4f-78d1dd9781b9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: openpyxl in c:\\users\\jemil\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (3.1.5)\n",
      "Requirement already satisfied: et-xmlfile in c:\\users\\jemil\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from openpyxl) (2.0.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 25.1.1 -> 25.2\n",
      "[notice] To update, run: C:\\Users\\jemil\\AppData\\Local\\Microsoft\\WindowsApps\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "pip install openpyxl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e0d35566-abe2-4032-a4c8-d5c0925310dc",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'dbutils' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[5]\u001b[39m\u001b[32m, line 12\u001b[39m\n\u001b[32m      9\u001b[39m OUT_DIR = (BASE_DIR / USER_SUB).as_posix()\n\u001b[32m     11\u001b[39m \u001b[38;5;66;03m# Crear carpeta en Volumes con dbutils (no Path.mkdir)\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m12\u001b[39m \u001b[43mdbutils\u001b[49m.fs.mkdirs(OUT_DIR)\n\u001b[32m     14\u001b[39m \u001b[38;5;66;03m# Verifica\u001b[39;00m\n\u001b[32m     15\u001b[39m display(dbutils.fs.ls(BASE_DIR.as_posix()))\n",
      "\u001b[31mNameError\u001b[39m: name 'dbutils' is not defined"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "VOLUME_NAME = \"lab8\"\n",
    "BASE_DIR = Path(\"/Volumes/workspace/default\") / VOLUME_NAME\n",
    "\n",
    "INPUT_XLSX = (BASE_DIR / \"Bases de datos principales PNC.xlsx\").as_posix()\n",
    "\n",
    "USER_SUB = \"silvia\"\n",
    "OUT_DIR = (BASE_DIR / USER_SUB).as_posix()\n",
    "\n",
    "# Crear carpeta en Volumes con dbutils (no Path.mkdir)\n",
    "dbutils.fs.mkdirs(OUT_DIR)\n",
    "\n",
    "# Verifica\n",
    "display(dbutils.fs.ls(BASE_DIR.as_posix()))\n",
    "print(\"Excel:\", INPUT_XLSX)\n",
    "print(\"OUT_DIR:\", OUT_DIR)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ffc372f2-d156-4f24-9409-f6ac73c6ce0b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Mostrar cuántos registros hay en cada tabla (hechos, vehículos, fallecidos, lesionados). Muestre algunos resultados con la función .show(). Genere un describe y summary para aquellas columnas que considere importantes según cada archivo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "edad3f67-58b8-401b-b6ea-7f2ca1399e00",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'dbutils' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 4\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mre\u001b[39;00m,\u001b[38;5;250m \u001b[39m\u001b[34;01mrandom\u001b[39;00m\n\u001b[32m      3\u001b[39m random.seed(\u001b[32m42\u001b[39m)\n\u001b[32m----> \u001b[39m\u001b[32m4\u001b[39m files = {f.path.split(\u001b[33m\"\u001b[39m\u001b[33m/\u001b[39m\u001b[33m\"\u001b[39m)[-\u001b[32m1\u001b[39m].lower(): f.path \u001b[38;5;28;01mfor\u001b[39;00m f \u001b[38;5;129;01min\u001b[39;00m \u001b[43mdbutils\u001b[49m.fs.ls(\u001b[38;5;28mstr\u001b[39m(OUT_DIR))}\n\u001b[32m      6\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mfind_csv_for\u001b[39m(i: \u001b[38;5;28mint\u001b[39m):\n\u001b[32m      7\u001b[39m     candidates = [\n\u001b[32m      8\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mcuadro \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mi\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m.csv\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m      9\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mcuadro_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mi\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m.csv\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m     10\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mCuadro \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mi\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m.csv\u001b[39m\u001b[33m\"\u001b[39m.lower(),\n\u001b[32m     11\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mCuadro_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mi\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m.csv\u001b[39m\u001b[33m\"\u001b[39m.lower(),\n\u001b[32m     12\u001b[39m     ]\n",
      "\u001b[31mNameError\u001b[39m: name 'dbutils' is not defined"
     ]
    }
   ],
   "source": [
    "import re, random\n",
    "\n",
    "random.seed(42)\n",
    "files = {f.path.split(\"/\")[-1].lower(): f.path for f in dbutils.fs.ls(str(OUT_DIR))}\n",
    "\n",
    "def find_csv_for(i: int):\n",
    "    candidates = [\n",
    "        f\"cuadro {i}.csv\",\n",
    "        f\"cuadro_{i}.csv\",\n",
    "        f\"Cuadro {i}.csv\".lower(),\n",
    "        f\"Cuadro_{i}.csv\".lower(),\n",
    "    ]\n",
    "    for c in candidates:\n",
    "        if c.lower() in files:\n",
    "            return files[c.lower()]\n",
    "    return None\n",
    "\n",
    "# --- Función para limpiar nombres de columnas ---\n",
    "def normalizar_cols(df):\n",
    "    nuevos = []\n",
    "    vistos = set()\n",
    "    for i, c in enumerate(df.columns):\n",
    "        name = str(c) if c is not None else f\"col_{i+1}\"\n",
    "        # quitar espacios adelante/atrás\n",
    "        name = name.strip()\n",
    "        # reemplazar espacios por \"_\"\n",
    "        name = re.sub(r\"\\s+\", \"_\", name)\n",
    "        # quitar caracteres no alfanuméricos\n",
    "        name = re.sub(r\"[^0-9A-Za-z_]\", \"\", name)\n",
    "        # evitar nombres vacíos\n",
    "        if not name:\n",
    "            name = f\"col_{i+1}\"\n",
    "        # asegurar que no se repitan nombres\n",
    "        base = name\n",
    "        k = 1\n",
    "        while name in vistos:\n",
    "            k += 1\n",
    "            name = f\"{base}_{k}\"\n",
    "        vistos.add(name)\n",
    "        nuevos.append(name)\n",
    "    return df.toDF(*nuevos)\n",
    "\n",
    "# 2) Cargar todos los que existan del 1 al 65\n",
    "dfs = {}         \n",
    "counts = {}\n",
    "loaded_names = [] \n",
    "\n",
    "for i in range(1, 66):\n",
    "    name = f\"cuadro {i}\"\n",
    "    path = find_csv_for(i)\n",
    "    if not path:\n",
    "        print(f\"[WARN] No encontré CSV para {name} en {OUT_DIR}\")\n",
    "        continue\n",
    "    try:\n",
    "        df = (spark.read.format(\"csv\")\n",
    "              .option(\"header\",\"true\")\n",
    "              .option(\"inferSchema\",\"true\")\n",
    "              .load(path))\n",
    "        # normalizar columnas\n",
    "        df = normalizar_cols(df)\n",
    "        dfs[name] = df\n",
    "        cnt = df.count()\n",
    "        counts[name] = cnt\n",
    "        loaded_names.append(name)\n",
    "        print(f\"[OK] Cargado {name}: {cnt} filas, {len(df.columns)} columnas -> {path}\")\n",
    "    except Exception as e:\n",
    "        print(f\"[ERR] No se pudo cargar {name} desde {path}: {e}\")\n",
    "\n",
    "if not loaded_names:\n",
    "    displayHTML(\"<p style='color:#c00'>No se cargó ningún cuadro. Revisa los nombres de archivos en OUT_DIR.</p>\")\n",
    "\n",
    "# 3) Elegir 5 al azar (o menos si hay menos de 5 cargados)\n",
    "k = min(5, len(loaded_names))\n",
    "sampled = random.sample(loaded_names, k) if k > 0 else []\n",
    "\n",
    "# 4) Mostrar 5 tablas aleatorias con título + conteo + vista de 10 filas\n",
    "for name in sampled:\n",
    "    df = dfs[name]\n",
    "    n  = counts[name]\n",
    "    displayHTML(f\"<h3 style='margin:10px 0'>{name.title()}</h3>\"\n",
    "                f\"<p style='margin:0 0 8px 0'>Registros: <b>{n}</b></p>\")\n",
    "    display(df.limit(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1f207b84-d470-4667-8d46-59fc2ee945d1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'sampled' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 95\u001b[39m\n\u001b[32m     92\u001b[39m     display(df.limit(n))\n\u001b[32m     94\u001b[39m \u001b[38;5;66;03m# Qué mostramos: si ya tienes 'sampled', úsalo; si no, usa hasta 5 de loaded_names\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m95\u001b[39m a_mostrar = sampled \u001b[38;5;28;01mif\u001b[39;00m \u001b[43msampled\u001b[49m \u001b[38;5;28;01melse\u001b[39;00m loaded_names[:\u001b[32m5\u001b[39m]\n\u001b[32m     97\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m name \u001b[38;5;129;01min\u001b[39;00m a_mostrar:\n\u001b[32m     98\u001b[39m     df = dfs[name]\n",
      "\u001b[31mNameError\u001b[39m: name 'sampled' is not defined"
     ]
    }
   ],
   "source": [
    "# ======================= MUESTRA DINÁMICA (reemplaza las 4 llamadas fijas) =======================\n",
    "TITULOS = {\n",
    "    1: \"Accidentes de tránsito ocurridos en la República de Guatemala, por año, según departamento. Serie histórica 2020 - 2024.\",\n",
    "    2: \"Accidentes de tránsito ocurridos en la República de Guatemala, por mes, según departamento, año 2024.\",\n",
    "    3: \"Accidentes de tránsito ocurridos en la República de Guatemala, por día de la semana, según departamento, año 2024.\",\n",
    "    4: \"Accidentes de tránsito ocurridos en la República de Guatemala, por año, según mes. Serie histórica 2020 - 2024.\",\n",
    "    5: \"Accidentes de tránsito ocurridos en la República de Guatemala, por día de la semana, según mes, año 2024.\",\n",
    "    6: \"Accidentes de tránsito ocurridos en la República de Guatemala, por año, según día de la semana. Serie histórica 2020 - 2024.\",\n",
    "    7: \"Accidentes de tránsito ocurridos en la República de Guatemala, por día de la semana, según hora de ocurrencia, año 2024.\",\n",
    "    8: \"Accidentes de tránsito ocurridos en la República de Guatemala, por tipo de accidente, según departamento, año 2024.\",\n",
    "    9: \"Accidentes de tránsito ocurridos en la República de Guatemala, por tipo de accidente, según mes, año 2024.\",\n",
    "    10: \"Accidentes de tránsito ocurridos en la República de Guatemala, por tipo de accidente, según día de ocurrencia, año 2024.\",\n",
    "    11: \"Accidentes de tránsito ocurridos por atropello en la República de Guatemala, por mes, según departamento, año 2024.\",\n",
    "    12: \"Accidentes de tránsito ocurridos por atropello en la República de Guatemala, por día de la semana, según departamento, año 2024.\",\n",
    "    13: \"Accidentes de tránsito ocurridos por atropello en la República de Guatemala, por día de la semana, según mes, año 2024.\",\n",
    "    14: \"Accidentes de tránsito ocurridos en el municipio de Guatemala, por zona de ocurrencia, según hora, año 2024.\",\n",
    "    15: \"Accidentes de tránsito ocurridos en el municipio de Guatemala, por día de la semana, según zona de ocurrencia, año 2024.\",\n",
    "    16: \"Accidentes de tránsito ocurridos en el municipio de Guatemala, por tipo de accidente, según zona de ocurrencia, año 2024.\",\n",
    "    17: \"Cantidad de vehículos involucrados en accidentes de tránsito ocurridos en la República de Guatemala, por año, según departamento. Serie histórica 2020 -2024.\",\n",
    "    18: \"Cantidad de vehículos involucrados en accidentes de tránsito ocurridos en la República de Guatemala, por tipo de accidente, según tipo de vehículo, año 2024.\",\n",
    "    19: \"Cantidad de vehículos involucrados en accidentes de tránsito ocurridos en la República de Guatemala, por tipo de accidente, según color de vehículo, año 2024.\",\n",
    "    20: \"Cantidad de vehículos involucrados en accidentes de tránsito ocurridos en la República de Guatemala, por tipo de accidente, según modelo de vehículo, año 2024.\",\n",
    "    21: \"Cantidad de vehículos involucrados en accidentes de tránsito ocurridos en la República de Guatemala, por tipo de vehículo, según departamento, año 2024.\",\n",
    "    22: \"Cantidad de vehículos involucrados en accidentes de tránsito ocurridos en la República de Guatemala, por sexo y condición del conductor, según departamento, año 2024.\",\n",
    "    23: \"Cantidad de vehículos involucrados en accidentes de tránsito ocurridos en la República de Guatemala, por sexo y condición del conductor, según mes, año 2024.\",\n",
    "    24: \"Cantidad de vehículos involucrados en accidentes de tránsito ocurridos en la República de Guatemala, por día de la semana, según condición del conductor y sexo, año 2024.\",\n",
    "    25: \"Cantidad de vehículos involucrados en accidentes de tránsito ocurridos en la República de Guatemala, por sexo y condición del conductor, según hora, año 2024.\",\n",
    "    26: \"Cantidad de vehículos involucrados en accidentes de tránsito ocurridos en la República de Guatemala, por sexo y condición del conductor, según grupos de edad, año 2024\",\n",
    "    27: \"Cantidad de vehículos involucrados en accidentes de tránsito ocurridos en la República de Guatemala, por tipo de accidente, según tipo de vehículo y sexo, año 2024.\",\n",
    "    28: \"Cantidad de vehículos involucrados en accidentes de tránsito ocurridos en el municipio de Guatemala, por tipo de vehículo, según zona de ocurrencia, año 2024.\",\n",
    "    29: \"Víctimas por accidentes de tránsito ocurridos en la República de Guatemala por año, según departamento. Serie histórica 2020 - 2024.\",\n",
    "    30: \"Víctimas por accidentes de tránsito ocurridos en la República de Guatemala, por sexo y estado del implicado, según departamento, año 2024.\",\n",
    "    31: \"Lesionados en accidentes de tránsito ocurridos en la República de Guatemala, por año, según departamento. Serie histórica 2020 - 2024.\",\n",
    "    32: \"Lesionados en accidentes de tránsito ocurridos en la República de Guatemala, por mes, según departamento, año 2024.\",\n",
    "    33: \"Lesionados en accidentes de tránsito ocurridos en la República de Guatemala, por día de la semana, según departamento, año 2024.\",\n",
    "    34: \"Lesionados en accidentes de tránsito ocurridos en la República de Guatemala, por grupos de edad, según departamento, año 2024.\",\n",
    "    35: \"Lesionados en accidentes de tránsito ocurridos en la República de Guatemala, por día de la semana, según hora, año 2024.\",\n",
    "    36: \"Lesionados en accidentes de tránsito ocurridos en la República de Guatemala, por sexo y tipo de accidente, según tipo de vehículo, año 2024.\",\n",
    "    37: \"Lesionados en accidentes de tránsito ocurridos en la República de Guatemala, por día de la semana, según tipo de accidente y sexo, año 2024.\",\n",
    "    38: \"Lesionados en accidentes de tránsito ocurridos en la República de Guatemala, por sexo, según grupos de edad, año 2024.\",\n",
    "    39: \"Lesionados por atropello en accidentes de tránsito ocurridos en la República de Guatemala, por mes, según departamento, año 2024.\",\n",
    "    40: \"Lesionados por atropello en accidentes de tránsito ocurridos en la República de Guatemala, por día de la semana, según departamento, año 2024.\",\n",
    "    41: \"Lesionados por atropello en accidentes de tránsito ocurridos en la República de Guatemala, por grupos de edad, según departamento, año 2024.\",\n",
    "    42: \"Lesionados por atropello en accidentes de tránsito ocurridos en la República de Guatemala, por día de la semana, según hora, año 2024.\",\n",
    "    43: \"Lesionados por atropello en accidentes de tránsito ocurridos en la República de Guatemala, por sexo, según tipo de vehículo, año 2024.\",\n",
    "    44: \"Lesionados en accidentes de tránsito ocurridos en el municipio de Guatemala, por grupos de edad, según zona de ocurrencia, año 2024.\",\n",
    "    45: \"Lesionados en accidentes de tránsito ocurridos en el municipio de Guatemala, por día de la semana, según zona de ocurrencia, año 2024.\",\n",
    "    46: \"Lesionados en accidentes de tránsito ocurridos en el municipio de Guatemala, por sexo, según zona de ocurrencia, año 2024.\",\n",
    "    47: \"Fallecidos en accidentes de tránsito ocurridos en la República de Guatemala, por año, según departamento. Serie histórica 2020 - 2024.\",\n",
    "    48: \"Fallecidos en accidentes de tránsito ocurridos en la República de Guatemala, por mes, según departamento, año 2024.\",\n",
    "    49: \"Fallecidos en accidentes de tránsito ocurridos en la República de Guatemala, por día de la semana, según departamento, año 2024.\",\n",
    "    50: \"Fallecidos en accidentes de tránsito ocurridos en la República de Guatemala, por grupos de edad, según departamento, año 2024.\",\n",
    "    51: \"Fallecidos en accidentes de tránsito ocurridos en la República de Guatemala, por día de la semana, según hora, año 2024.\",\n",
    "    52: \"Fallecidos en accidentes de tránsito ocurridos en la República de Guatemala, por sexo y tipo de accidente, según tipo de vehículo, año 2024.\",\n",
    "    53: \"Fallecidos en accidentes de tránsito ocurridos en la República de Guatemala, por día de la semana, según tipo de accidente y sexo, año 2024.\",\n",
    "    54: \"Fallecidos en accidentes de tránsito ocurridos en la República de Guatemala, por sexo, según grupos de edad, año 2024.\",\n",
    "    55: \"Fallecidos por atropello en accidentes de tránsito ocurridos en la República de Guatemala, por mes, según departamento, año 2024.\",\n",
    "    56: \"Fallecidos por atropello en accidentes de tránsito ocurridos en la República de Guatemala, por día de la semana, según departamento, año 2024.\",\n",
    "    57: \"Fallecidos por atropello en accidentes de tránsito ocurridos en la República de Guatemala, por grupos de edad, según departamento, año 2024.\",\n",
    "    58: \"Fallecidos por atropello en accidentes de tránsito ocurridos en la República de Guatemala, por día de la semana, según hora, año 2024.\",\n",
    "    59: \"Fallecidos por atropello en accidentes de tránsito ocurridos en la República de Guatemala, por sexo, según tipo de vehículo, año 2024.\",\n",
    "    60: \"Fallecidos en accidentes de tránsito ocurridos en el municipio de Guatemala, por grupos de edad, según zona de ocurrencia, año 2024.\",\n",
    "    61: \"Fallecidos en accidentes de tránsito ocurridos en el municipio de Guatemala, por día de la semana, según zona de ocurrencia, año 2024.\",\n",
    "    62: \"Fallecidos en accidentes de tránsito ocurridos en el municipio de Guatemala, por sexo, según zona de ocurrencia, año 2024.\",\n",
    "    63: \"Tasa de víctimas involucradas en accidentes de tránsito ocurridos en la República de Guatemala por año, según departamento por cada 100,000 habitantes. Serie histórica 2020 - 2024.\",\n",
    "    64: \"Tasa de víctimas lesionadas involucradas en accidentes de tránsito ocurridos en la República de Guatemala por año, según departamento por cada 100,000 habitantes. Serie histórica 2020 – 2024.\",\n",
    "    65: \"Tasa de víctimas fallecidas involucradas en accidentes de tránsito ocurridos en la República de Guatemala por año, según departamento por cada 100,000 habitantes. Serie histórica 2020 - 2024.\"\n",
    "}\n",
    "\n",
    "import re\n",
    "\n",
    "def _titulo_para(name: str) -> str:\n",
    "    # name viene como \"cuadro 12\", extraemos el índice para buscar en TITULOS\n",
    "    m = re.search(r\"(\\d+)$\", name.strip())\n",
    "    if m:\n",
    "        idx = int(m.group(1))\n",
    "        return TITULOS.get(idx, f\"Cuadro {idx}\")\n",
    "    return name.title()\n",
    "\n",
    "from pyspark.sql import functions as F\n",
    "\n",
    "def show_sample(df, titulo, n=10, cols=None, truncate=False):\n",
    "    displayHTML(f\"<h4 style='margin:6px 0'>{titulo} — muestra de {n} filas</h4>\")\n",
    "    # Si nos pasan columnas, filtramos a las que existan\n",
    "    if cols:\n",
    "        if isinstance(cols, str):\n",
    "            cols = [c.strip() for c in cols.split(\",\")]\n",
    "        keep = [c for c in cols if c in df.columns]\n",
    "        if keep:\n",
    "            df = df.select(*[F.col(c) for c in keep])\n",
    "    # En Databricks, display(df.limit(n)) es más cómodo que df.show()\n",
    "    display(df.limit(n))\n",
    "\n",
    "a_mostrar = sampled if sampled else loaded_names[:5]\n",
    "\n",
    "for name in a_mostrar:\n",
    "    df = dfs[name]\n",
    "    titulo = _titulo_para(name)\n",
    "    show_sample(df, titulo, n=10)\n",
    "# ================================================================================================"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2c0ddadd-1fb3-4a6e-8887-25f5e47ec6a0",
     "showTitle": false,
     "tableResultSettingsMap": {
      "1": {
       "dataGridStateBlob": "{\"version\":1,\"tableState\":{\"columnPinning\":{\"left\":[\"#row_number#\"],\"right\":[]},\"columnSizing\":{},\"columnVisibility\":{}},\"settings\":{\"columns\":{}},\"syncTimestamp\":1759398910755}",
       "filterBlob": null,
       "queryPlanFiltersBlob": null,
       "tableResultIndex": 1
      }
     },
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def _idx(name: str) -> int:\n",
    "    m = re.search(r\"(\\d+)$\", name.strip())\n",
    "    return int(m.group(1)) if m else 10**9\n",
    "\n",
    "numeric_primitives = (\"byte\", \"short\", \"int\", \"bigint\", \"long\", \"float\", \"double\")\n",
    "\n",
    "for name in sorted(loaded_names, key=_idx):\n",
    "    df = dfs[name]\n",
    "\n",
    "    # Detect numeric columns (including decimal with any precision/scale)\n",
    "    num_cols = [\n",
    "        c for c, t in df.dtypes\n",
    "        if t in numeric_primitives or t.lower().startswith(\"decimal\")\n",
    "    ]\n",
    "\n",
    "    displayHTML(f\"<h4 style='margin:10px 0'>{name.title()} — columnas numéricas</h4>\")\n",
    "    if not num_cols:\n",
    "        displayHTML(\"<p style='margin:0 0 8px 0'><i>Sin columnas numéricas detectadas.</i></p>\")\n",
    "        continue\n",
    "\n",
    "    # Ensure all columns exist in DataFrame\n",
    "    existing_num_cols = [c for c in num_cols if c in df.columns]\n",
    "    if not existing_num_cols:\n",
    "        displayHTML(\"<p style='margin:0 0 8px 0'><i>No valid numeric columns found in DataFrame.</i></p>\")\n",
    "        continue\n",
    "\n",
    "    # Show statistical summary of numeric columns\n",
    "    displayHTML(\"<p style='margin:6px 0 4px 0'><i>Resumen (describe):</i></p>\")\n",
    "    display(\n",
    "        df.select(*existing_num_cols).describe()\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bf073d46-456b-423b-abaf-4e99cc56156e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import Row\n",
    "\n",
    "percentiles = [0.25, 0.5, 0.75]\n",
    "rel_tol = 0.01  # tolerancia de approxQuantile\n",
    "\n",
    "for name in sorted(loaded_names, key=_idx):\n",
    "    df = dfs[name]\n",
    "    num_cols = [c for c, t in df.dtypes\n",
    "                if t in numeric_primitives or t.lower().startswith(\"decimal\")]\n",
    "\n",
    "    if not num_cols:\n",
    "        continue\n",
    "\n",
    "    displayHTML(f\"<h4 style='margin:10px 0'>{name.title()} — percentiles</h4>\")\n",
    "    rows = []\n",
    "    for c in num_cols:\n",
    "        qs = df.approxQuantile(c, percentiles, rel_tol)\n",
    "        rows.append(Row(col=c, p25=qs[0], p50=qs[1], p75=qs[2]))\n",
    "    spark.createDataFrame(rows).show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e31da645-f7a2-455e-9860-e485889f37c7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#2. Identificar los años disponibles en cada tabla y validar si coinciden."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bf232a9b-caca-4ece-8417-c75e84ab6cb4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# ===================== DETECCIÓN DE AÑOS EN TODAS LAS TABLAS (1–65) =====================\n",
    "def _idx(name: str) -> int:\n",
    "    m = re.search(r\"(\\d+)$\", name.strip())\n",
    "    return int(m.group(1)) if m else 10**9\n",
    "\n",
    "YEAR_COL_REGEX = re.compile(\n",
    "    r\"^(anio|ano|a[o]?|year|anio_?hecho|ano_?hecho|anio_?ocurrencia|ano_?ocurrencia|anio_?registro)$\",\n",
    "    re.I\n",
    ")\n",
    "\n",
    "YEAR_MIN, YEAR_MAX = 1990, 2035\n",
    "\n",
    "def _years_from_candidate_cols(df):\n",
    "    years = set()\n",
    "    candidates = [c for c in df.columns if YEAR_COL_REGEX.search(c)]\n",
    "    for c in candidates:\n",
    "        ydf = (df\n",
    "               .select(F.col(c).cast(\"int\").alias(\"y\"))\n",
    "               .where(F.col(\"y\").isNotNull() & (F.col(\"y\") >= YEAR_MIN) & (F.col(\"y\") <= YEAR_MAX))\n",
    "               .select(\"y\").distinct())\n",
    "        years.update([r[\"y\"] for r in ydf.collect()])\n",
    "    return years\n",
    "\n",
    "def _years_from_headers(df):\n",
    "    \"\"\"Si la tabla está en formato ancho (años como columnas), detecta encabezados 'YYYY' válidos.\"\"\"\n",
    "    years = set()\n",
    "    for c in df.columns:\n",
    "        cc = c.strip()\n",
    "        if re.fullmatch(r\"\\d{4}\", cc):\n",
    "            y = int(cc)\n",
    "            if YEAR_MIN <= y <= YEAR_MAX:\n",
    "                years.add(y)\n",
    "    return years\n",
    "\n",
    "def detectar_anios(df):\n",
    "    \"\"\"Intenta por columnas de año; si no hay, intenta por encabezados.\"\"\"\n",
    "    y1 = _years_from_candidate_cols(df)\n",
    "    if y1:\n",
    "        return sorted(y1)\n",
    "    y2 = _years_from_headers(df)\n",
    "    if y2:\n",
    "        return sorted(y2)\n",
    "    return []\n",
    "\n",
    "displayHTML(\"<h3 style='margin:8px 0'>Años disponibles por tabla</h3>\")\n",
    "\n",
    "anios_por_tabla = {}\n",
    "problemas_fuera_de_rango = []\n",
    "\n",
    "for name in sorted(loaded_names, key=_idx):\n",
    "    df = dfs[name]\n",
    "    anios = detectar_anios(df)\n",
    "    anios_por_tabla[name] = anios if anios else None\n",
    "    lista = \", \".join(map(str, anios)) if anios else \"<i>No se encontraron años</i>\"\n",
    "    displayHTML(f\"<p style='margin:4px 0'><b>{name.title()}:</b> {lista}</p>\")\n",
    "\n",
    "# --- Validación global ---\n",
    "validas = [set(v) for v in anios_por_tabla.values() if v]\n",
    "\n",
    "if validas:\n",
    "    all_equal = all(s == validas[0] for s in validas)\n",
    "    inter = sorted(list(set.intersection(*validas))) if len(validas) > 1 else sorted(list(validas[0]))\n",
    "    uni = sorted(list(set.union(*validas)))        if len(validas) > 1 else sorted(list(validas[0]))\n",
    "\n",
    "    displayHTML(\"<h3 style='margin:12px 0 4px 0'>Validación</h3>\")\n",
    "    displayHTML(f\"<p style='margin:0'><b>¿Todos iguales?</b> {'Sí' if all_equal else 'No'}</p>\")\n",
    "    displayHTML(f\"<p style='margin:0'><b>Intersección:</b> {', '.join(map(str, inter)) if inter else '—'}</p>\")\n",
    "    displayHTML(f\"<p style='margin:0'><b>Unión:</b> {', '.join(map(str, uni)) if uni else '—'}</p>\")\n",
    "else:\n",
    "    displayHTML(\"<p style='margin:8px 0'><i>No hay tablas con años detectados para comparar.</i></p>\")\n",
    "#=============================================================================="
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "81a7c166-c49d-4523-a41d-a9b5b27fb90b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# 3. Mostrar los valores distintos de tipo de accidente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c159fb0a-1acf-4014-9cfb-9d54349de8ce",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# ===================== TIPOS DE ACCIDENTE (valores distintos) =====================\n",
    "import re\n",
    "from pyspark.sql import functions as F\n",
    "\n",
    "# --- 1) identificar qué cuadros \"van de tipo de accidente\" a partir del diccionario ---\n",
    "diccionario = TITULOS  \n",
    "cuadros_tipo = sorted([i for i, desc in diccionario.items() if \"tipo de accidente\" in desc.lower()])\n",
    "\n",
    "# --- 2) normalizador simple de columnas (igual al que usamos antes) ---\n",
    "import re as _re\n",
    "def normalizar_cols(df):\n",
    "    nuevos, vistos = [], set()\n",
    "    for i, c in enumerate(df.columns):\n",
    "        name = str(c) if c is not None else f\"col_{i+1}\"\n",
    "        name = name.strip()\n",
    "        name = _re.sub(r\"\\s+\", \"_\", name)\n",
    "        name = _re.sub(r\"[^0-9A-Za-z_]\", \"\", name)\n",
    "        if not name:\n",
    "            name = f\"col_{i+1}\"\n",
    "        base, k = name, 1\n",
    "        while name in vistos:\n",
    "            k += 1\n",
    "            name = f\"{base}_{k}\"\n",
    "        vistos.add(name)\n",
    "        nuevos.append(name)\n",
    "    return df.toDF(*nuevos)\n",
    "\n",
    "# --- 3) helpers de detección ---\n",
    "RE_TIPO_COL = re.compile(r\"(tipo.*acciden|acciden.*tipo)\", re.I)\n",
    "\n",
    "# columnas de dimensión / medidas a excluir cuando la tabla esté \"ancha\"\n",
    "EXCLUDE_HEADERS = set([\n",
    "    \"departamento\",\"depto\",\"municipio\",\"zona\",\"mes\",\"dia\",\"hora\",\n",
    "    \"anio\",\"ano\",\"ao\",\"anio_hecho\",\"ano_hecho\",\"anio_ocurrencia\",\"ano_ocurrencia\",\"anio_registro\",\n",
    "    \"sexo\",\"condicion\",\"estado\",\"grupos_de_edad\",\"grupo_de_edad\",\"grupo_edad\",\"edad\",\n",
    "    \"tipo_de_vehiculo\",\"tipo_vehiculo\",\"vehiculo\",\"color\",\"modelo\",\n",
    "    \"total\",\"cantidad\",\"conteo\",\"victimas\",\"lesionados\",\"fallecidos\"\n",
    "])\n",
    "\n",
    "YEAR_MIN, YEAR_MAX = 1990, 2035\n",
    "\n",
    "def _is_year_col(name: str) -> bool:\n",
    "    return bool(re.fullmatch(r\"\\d{4}\", name)) and (YEAR_MIN <= int(name) <= YEAR_MAX)\n",
    "\n",
    "def _idx(name: str) -> int:\n",
    "    m = re.search(r\"(\\d+)$\", name.strip())\n",
    "    return int(m.group(1)) if m else 10**9\n",
    "\n",
    "# --- 4) recorrer y mostrar valores distintos ---\n",
    "displayHTML(\"<h3 style='margin:8px 0'>Valores distintos de <i>tipo de accidente</i></h3>\")\n",
    "\n",
    "tipos_global = set()\n",
    "\n",
    "for i in cuadros_tipo:\n",
    "    name = f\"cuadro {i}\"\n",
    "    if name not in dfs:\n",
    "        displayHTML(f\"<p><b>{name.title()}:</b> <i>No cargado.</i></p>\")\n",
    "        continue\n",
    "\n",
    "    df_raw = dfs[name]\n",
    "    df = normalizar_cols(df_raw)\n",
    "\n",
    "    # a) intentar columna explícita\n",
    "    tipo_col = None\n",
    "    for c in df.columns:\n",
    "        if RE_TIPO_COL.search(c):\n",
    "            tipo_col = c\n",
    "            break\n",
    "\n",
    "    tipos = []\n",
    "    if tipo_col:\n",
    "        # columna encontrada -> extraer valores distintos (string limpio, sin nulos)\n",
    "        vals = (df.select(F.col(tipo_col).cast(\"string\").alias(\"tipo\"))\n",
    "                  .where(F.col(\"tipo\").isNotNull() & (F.length(F.trim(\"tipo\")) > 0))\n",
    "                  .select(F.trim(\"tipo\").alias(\"tipo\")).distinct()\n",
    "                  .orderBy(\"tipo\"))\n",
    "        tipos = [r[\"tipo\"] for r in vals.collect()]\n",
    "    else:\n",
    "        # b) formato ancho: los encabezados son los tipos\n",
    "        # candidatos = columnas que NO sean año, NO estén en EXCLUDE_HEADERS y NO sean 4 dígitos\n",
    "        candidatos = []\n",
    "        for c in df.columns:\n",
    "            cl = c.lower()\n",
    "            if _is_year_col(c):\n",
    "                continue\n",
    "            if cl in EXCLUDE_HEADERS:\n",
    "                continue\n",
    "            # si parece claramente métrica numérica genérica, saltar (opcional)\n",
    "            if cl in (\"valor\",\"valores\",\"monto\",\"cantidad_total\"):\n",
    "                continue\n",
    "            candidatos.append(c)\n",
    "\n",
    "        # si la tabla es bien ancha, estos suelen ser los tipos\n",
    "        # Validación ligera: si hay muy pocos candidatos y además ninguna col de \"tipo\", mostramos aviso\n",
    "        tipos = sorted(candidatos)\n",
    "\n",
    "    if tipos:\n",
    "        tipos_global.update(tipos)\n",
    "        lista = \", \".join(tipos)\n",
    "        displayHTML(f\"<p style='margin:4px 0'><b>{name.title()}:</b> {lista}</p>\")\n",
    "    else:\n",
    "        displayHTML(f\"<p style='margin:4px 0'><b>{name.title()}:</b> <i>No se detectaron tipos</i></p>\")\n",
    "\n",
    "# --- 5) resumen global ---\n",
    "if tipos_global:\n",
    "    displayHTML(\"<h4 style='margin:12px 0 6px 0'>Resumen global (únicos)</h4>\")\n",
    "    displayHTML(\"<p style='margin:0'>\" + \", \".join(sorted(tipos_global)) + \"</p>\")\n",
    "else:\n",
    "    displayHTML(\"<p style='margin:12px 0'><i>No se detectaron tipos de accidente en los cuadros identificados.</i></p>\")\n",
    "# ============================================================================================="
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ba02ff7e-4c3c-45d5-b99f-4c2e406b63c7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#4. Calcular cuántos departamentos únicos aparecen en las bases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ef5970af-b741-44c7-88a0-6d09f0cd37f8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# ===================== DEPARTAMENTOS ÚNICOS (filtrando por TITULOS) =====================\n",
    "import re\n",
    "from pyspark.sql import functions as F\n",
    "\n",
    "# 1) Identificar qué cuadros mencionan \"departamento\" en el título/descripción\n",
    "cuadros_con_departamento = sorted(\n",
    "    [i for i, txt in TITULOS.items() if re.search(r\"departament\", txt, re.I)]\n",
    ")\n",
    "\n",
    "# 2) Helper: localizar la columna de departamento en cada DF\n",
    "DEPTO_COL_REGEX = re.compile(r\"^(depto|departamento|departament|dpto|dep)$\", re.I)\n",
    "\n",
    "def encontrar_col_departamento(df):\n",
    "    for c in df.columns:\n",
    "        if DEPTO_COL_REGEX.search(c.strip()):\n",
    "            return c\n",
    "    return None\n",
    "\n",
    "# 3) Recorrer solo esos cuadros y reunir valores únicos\n",
    "departamentos_global = set()\n",
    "\n",
    "for i in cuadros_con_departamento:\n",
    "    name = f\"cuadro {i}\"\n",
    "    if name not in dfs:\n",
    "        print(f\"[WARN] {name} no está cargado; se omite.\")\n",
    "        continue\n",
    "\n",
    "    df = dfs[name]\n",
    "    col = encontrar_col_departamento(df)\n",
    "    if not col:\n",
    "        print(f\"[INFO] {name} no tiene columna de departamento detectable; se omite.\")\n",
    "        continue\n",
    "\n",
    "    vals = (df.select(F.col(col).cast(\"string\").alias(\"depto\"))\n",
    "              .where(F.col(\"depto\").isNotNull() & (F.length(F.trim(\"depto\")) > 0))\n",
    "              .select(F.trim(\"depto\").alias(\"depto\")).distinct())\n",
    "    departamentos_global.update([r[\"depto\"] for r in vals.collect()])\n",
    "\n",
    "# 4) Mostrar resultados\n",
    "lista_departamentos = sorted(departamentos_global)\n",
    "displayHTML(\"<h3 style='margin:8px 0'>Departamentos únicos (solo cuadros que mencionan “departamento”)</h3>\")\n",
    "displayHTML(f\"<p style='margin:0 0 6px 0'><b>Total:</b> {len(lista_departamentos)}</p>\")\n",
    "displayHTML(\"<p>\" + \", \".join(lista_departamentos) + \"</p>\")\n",
    "# ========================================================================================="
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "4178fefe-fe29-4cf4-87d6-93204f0dc652",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# 5. ¿Cuál es el total de accidentes por año y departamento? Apóyese de la función groupBy. Investigue la función display que tiene Databricks y muestre su resultado en formato de gráfico de barras."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "97e388ca-58fb-4e1b-8b83-af4d61ceb360",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F\n",
    "import re\n",
    "\n",
    "df = dfs[\"cuadro 1\"]\n",
    "\n",
    "dept_col = next((c for c in df.columns if re.search(r\"(depto|depart|departamento)\", c, re.I)), df.columns[0])\n",
    "year_cols = [c for c in df.columns if re.fullmatch(r\"\\d{4}\", c)]\n",
    "\n",
    "df_total = df.filter(F.lower(F.col(dept_col)).like(\"total%\"))\n",
    "if df_total.count() == 0:\n",
    "    df_total = df.groupBy().agg(*[F.sum(F.col(c)).alias(c) for c in year_cols])\n",
    "\n",
    "df_total_long = (df_total\n",
    "    .select(\n",
    "        F.explode(\n",
    "            F.arrays_zip(\n",
    "                F.array(*[F.lit(c) for c in year_cols]),\n",
    "                F.array(*[F.col(c) for c in year_cols])\n",
    "            )\n",
    "        ).alias(\"kv\")\n",
    "    )\n",
    "    .select(F.col(\"kv.0\").alias(\"año\"), F.col(\"kv.1\").cast(\"int\").alias(\"total_accidentes\"))\n",
    "    .orderBy(\"año\"))\n",
    "\n",
    "display(df_total_long)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a737c75c-eaab-495d-b700-b9975f5ce072",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_dept = df.filter(~F.lower(F.col(dept_col)).like(\"total%\"))\n",
    "\n",
    "df_long = (df_dept\n",
    "    .select(\n",
    "        F.col(dept_col),\n",
    "        F.explode(\n",
    "            F.arrays_zip(\n",
    "                F.array(*[F.lit(c) for c in year_cols]),\n",
    "                F.array(*[F.col(c) for c in year_cols])\n",
    "            )\n",
    "        ).alias(\"kv\")\n",
    "    )\n",
    "    .select(\n",
    "        F.col(dept_col).alias(\"departamento\"),\n",
    "        F.col(\"kv.0\").alias(\"anio\"),\n",
    "        F.col(\"kv.1\").cast(\"int\").alias(\"accidentes\")\n",
    "    ))\n",
    "\n",
    "df_grouped = (df_long.groupBy(\"anio\", \"departamento\")\n",
    "                       .agg(F.sum(\"accidentes\").alias(\"total_accidentes\"))\n",
    "                       .orderBy(\"anio\", \"departamento\"))\n",
    "\n",
    "# Mostrar en Databricks\n",
    "display(df_grouped)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "0911a079-c4af-4c06-ae5c-3eb4aebc576a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# 6. ¿Qué día de la semana registra más accidentes en 2024? Graficar con display en un gráfico de columnas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "da1d0808-ba3a-4579-8001-f054ee7ce3f6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F\n",
    "import re, unicodedata\n",
    "\n",
    "df = dfs[\"cuadro 3\"]  # ajusta el nombre si difiere\n",
    "\n",
    "def strip_accents(s: str) -> str:\n",
    "    return \"\".join(c for c in unicodedata.normalize(\"NFD\", s or \"\") if unicodedata.category(c) != \"Mn\")\n",
    "\n",
    "def norm(s: str) -> str:\n",
    "    s = strip_accents(s).lower()\n",
    "    return re.sub(r\"[^a-z]\", \"\", s)\n",
    "\n",
    "# Variantes por día (para capturar 'Mircoles', 'Sbado', abreviaturas, etc.)\n",
    "day_variants = {\n",
    "    \"Lunes\":      [\"lun\"],\n",
    "    \"Martes\":     [\"mar\"],\n",
    "    \"Miercoles\":  [\"mie\",\"mir\"],   # 'mircoles' sin 'e'\n",
    "    \"Jueves\":     [\"jue\"],\n",
    "    \"Viernes\":    [\"vier\",\"vie\"],\n",
    "    \"Sabado\":     [\"sab\",\"sb\"],     # 'sbado' sin 'a'\n",
    "    \"Domingo\":    [\"dom\"],\n",
    "}\n",
    "\n",
    "# Detectar columna real por día\n",
    "colmap = {}\n",
    "for c in df.columns:\n",
    "    nk = norm(c)\n",
    "    for day, roots in day_variants.items():\n",
    "        if any((nk.startswith(r) or r in nk) for r in roots):\n",
    "            # Si hay empate, quédate con el match más largo (más específico)\n",
    "            if day not in colmap or len(nk) > len(norm(colmap[day])):\n",
    "                colmap[day] = c\n",
    "\n",
    "faltan = [d for d in day_variants if d not in colmap]\n",
    "if faltan:\n",
    "    raise ValueError(f\"No encontré todas las columnas de días. Faltan: {faltan}. Detectadas: {colmap}\")\n",
    "\n",
    "# Quitar la fila Total si existe\n",
    "col_dep = next((c for c in df.columns if \"depar\" in norm(c)), None)\n",
    "df_work = df.where(F.lower(F.col(col_dep).cast(\"string\")) != \"total\") if col_dep else df\n",
    "\n",
    "# Unpivot a formato largo\n",
    "pairs = []\n",
    "ordered_days = [\"Lunes\",\"Martes\",\"Miercoles\",\"Jueves\",\"Viernes\",\"Sabado\",\"Domingo\"]\n",
    "for d in ordered_days:\n",
    "    pairs += [f\"'{d}'\", f\"`{colmap[d]}`\"]\n",
    "stack_expr = f\"stack(7, {', '.join(pairs)}) as (dia, accidentes)\"\n",
    "\n",
    "df_long = (df_work.selectExpr(stack_expr)\n",
    "                   .select(\"dia\", F.col(\"accidentes\").cast(\"double\")))\n",
    "\n",
    "# Sumar 2024 por día\n",
    "df_dia = (df_long.groupBy(\"dia\")\n",
    "                 .agg(F.sum(F.coalesce(\"accidentes\", F.lit(0))).alias(\"total_accidentes\"))\n",
    "                 .orderBy(F.desc(\"total_accidentes\")))\n",
    "\n",
    "# Mostrar tabla (en la UI selecciona Bar chart: X=dia, Y=total_accidentes)\n",
    "display(df_dia)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ba599fa3-548b-4f30-a2c7-03c216bcebf8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# 7. Mostrar la distribución de accidentes por hora del día en el municipio de Guatemala. Graficar en un histograma."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f14eb574-28f1-47f3-8d15-87cd8d3ffe6d",
     "showTitle": false,
     "tableResultSettingsMap": {
      "0": {
       "dataGridStateBlob": "{\"version\":1,\"tableState\":{\"columnPinning\":{\"left\":[\"#row_number#\"],\"right\":[]},\"columnSizing\":{},\"columnVisibility\":{}},\"settings\":{\"columns\":{}},\"syncTimestamp\":1759403305624}",
       "filterBlob": "{\"version\":1,\"filterGroups\":[{\"enabled\":true,\"filterGroupId\":\"fg_4a0319c2\",\"op\":\"OR\",\"filters\":[{\"filterId\":\"f_59ce8c70\",\"enabled\":true,\"columnId\":\"Total\",\"dataType\":\"integer\",\"filterType\":\"oneof\"}],\"local\":false,\"updatedAt\":1759403137238},{\"enabled\":true,\"filterGroupId\":\"fg_45213e1a\",\"op\":\"OR\",\"filters\":[{\"filterId\":\"f_72064125\",\"enabled\":true,\"columnId\":\"Hora_de_ocurrencia\",\"dataType\":\"string\",\"filterType\":\"oneof\",\"filterConfig\":{}}],\"local\":false,\"updatedAt\":1759403148295}],\"syncTimestamp\":1759403148304}",
       "queryPlanFiltersBlob": "[]",
       "tableResultIndex": 0
      }
     },
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F\n",
    "\n",
    "# 1) Tomamos el cuadro 14\n",
    "df = dfs[\"cuadro 14\"]\n",
    "\n",
    "# 2) Quitamos columnas 'Total' e 'Ignorada' si existen\n",
    "cols_to_drop = [c for c in [\"Total\", \"Ignorada\"] if c in df.columns]\n",
    "df_clean = df.drop(*cols_to_drop)\n",
    "\n",
    "# 3) (Por si venían como filas) quitamos filas 'Ignorada' y 'Total'\n",
    "df_clean = df_clean.filter(~F.col(\"Hora_de_ocurrencia\").isin([\"Ignorada\", \"Total\"]))\n",
    "\n",
    "# 4) Sumamos todas las columnas numéricas por franja horaria\n",
    "num_cols = [c for c in df_clean.columns if c != \"Hora_de_ocurrencia\"]\n",
    "df_horas = (\n",
    "    df_clean\n",
    "    .withColumn(\n",
    "        \"total_accidentes\",\n",
    "        sum(F.coalesce(F.col(c).cast(\"double\"), F.lit(0.0)) for c in num_cols)\n",
    "    )\n",
    "    .select(\"Hora_de_ocurrencia\", \"total_accidentes\")\n",
    ")\n",
    "\n",
    "# 5) Ordenamos por la hora inicial (00, 01, 02, …)\n",
    "df_horas = (\n",
    "    df_horas\n",
    "    .withColumn(\"start_h\", F.regexp_extract(\"Hora_de_ocurrencia\", r\"^(\\d{2})\", 1).cast(\"int\"))\n",
    "    .orderBy(\"start_h\")\n",
    "    .drop(\"start_h\")\n",
    ")\n",
    "\n",
    "# 6) Mostrar y graficar con display (en la UI: Plot -> Bar -> Keys=Hora_de_ocurrencia, Values=total_accidentes)\n",
    "display(df_horas)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a4137329-022b-42be-8098-1f1caddfabe6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# 8. Unir la tabla de hechos de tránsito con la de vehículos usando una llavecompuesta por año, mes, departamento y tipo de accidente. ¿Cuántos registros combinados se logran?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c46d2d98-def6-417a-bb39-78a8232aa88a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "# hechos_long: [anio, mes, departamento (NULL), tipo_de_accidente, accidentes]\n",
    "# vehiculos_long: [anio, mes=NULL, departamento=NULL, tipo_de_accidente, num_vehiculos]\n",
    "\n",
    "joined_min = hechos_long.join(\n",
    "    vehiculos_long.select(\"anio\",\"tipo_de_accidente\",\"num_vehiculos\"),\n",
    "    on=[\"anio\",\"tipo_de_accidente\"], how=\"inner\"\n",
    ")\n",
    "\n",
    "conteo_min = joined_min.count()\n",
    "print(f\"Registros combinados por ['anio','tipo_de_accidente']: {conteo_min}\")\n",
    "joined_min.orderBy(\"anio\",\"tipo_de_accidente\").show(20, truncate=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "306a3816-4e3e-4e6b-8d65-f31d105a0417",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "9. De la unión anterior, calcular el promedio de vehículos por accidente en cada\n",
    "departamento. Guardar este resultado en formato Parquet. Luego, vuelva a cargarlo y\n",
    "grafique los 10 departamentos con más vehículos/accidente"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "298ae078-56d6-40c2-bbd0-65ce74bd6107",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F\n",
    "import re\n",
    "\n",
    "def _norm_cols(df):\n",
    "    try:\n",
    "        return normalizar_cols(df)\n",
    "    except NameError:\n",
    "        d = df\n",
    "        for c in d.columns:\n",
    "            d = d.withColumnRenamed(c, c.strip().lower().replace(\" \", \"_\"))\n",
    "        return d\n",
    "\n",
    "def _numeric_cols(df):\n",
    "    prim = (\"byte\",\"short\",\"int\",\"bigint\",\"long\",\"float\",\"double\")\n",
    "    return [c for c,t in df.dtypes if t in prim or t.lower().startswith(\"decimal\")]\n",
    "\n",
    "# Accidentes por departamento (2024) desde cuadro 1\n",
    "c1 = _norm_cols(dfs[\"cuadro 1\"])\n",
    "dept_c1 = next((c for c in c1.columns if re.search(r\"depto|depart\", c, re.I)), None)\n",
    "acc_2024 = (c1.select(F.col(dept_c1).alias(\"departamento\"), F.col(\"2024\").cast(\"double\").alias(\"accidentes\"))\n",
    "              .where(~F.col(\"departamento\").like(\"total%\"))\n",
    "              .withColumn(\"departamento\", F.lower(F.trim(F.col(\"departamento\")))))\n",
    "\n",
    "# Vehículos por departamento (sumando todos los tipos) desde cuadro 8\n",
    "c8 = _norm_cols(dfs[\"cuadro 8\"])\n",
    "dept_c8 = next((c for c in c8.columns if re.search(r\"depto|depart\", c, re.I)), None)\n",
    "num_cols_c8 = [c for c in _numeric_cols(c8) if c.lower() != \"total\"]\n",
    "veh_depto = (c8.select(F.col(dept_c8).alias(\"departamento\"), *num_cols_c8)\n",
    "               .where(~F.col(dept_c8).like(\"total%\")))\n",
    "veh_depto = (veh_depto\n",
    "             .withColumn(\"departamento\", F.lower(F.trim(F.col(\"departamento\"))))\n",
    "             .withColumn(\"vehiculos\", sum([F.col(c).cast(\"double\") for c in num_cols_c8])))\n",
    "\n",
    "# Join por departamento y ratio\n",
    "joined_dep = acc_2024.join(veh_depto.select(\"departamento\",\"vehiculos\"), on=\"departamento\", how=\"inner\")\n",
    "\n",
    "res_dep = (joined_dep\n",
    "           .select(\"departamento\", (F.col(\"vehiculos\")/F.col(\"accidentes\")).alias(\"vehiculos_por_acc\"))\n",
    "           .orderBy(F.col(\"vehiculos_por_acc\").desc()))\n",
    "\n",
    "# Guardar & mostrar\n",
    "out_path = str(BASE_DIR / \"Output/vehiculos_por_accidente.parquet\")\n",
    "res_dep.write.mode(\"overwrite\").parquet(out_path)\n",
    "res_reload = spark.read.parquet(out_path)\n",
    "\n",
    "print(f\"✅ Guardado en Parquet: {out_path}\")\n",
    "display(res_reload.orderBy(F.col(\"vehiculos_por_acc\").desc()).limit(10))  # grafícalo como barras en la UI\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "2db010b2-fc90-4a91-b609-510eba0676b8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "10. Encontrar el top 5 de colores de vehículos más involucrados en accidentes.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "040cd666-e807-4253-8261-0c43743037d0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# === PUNTO 10: Top 5 de colores de vehículos ===\n",
    "from pyspark.sql import functions as F\n",
    "\n",
    "# Cargar el cuadro que contiene la distribución de vehículos por color\n",
    "# Ajusta el número de cuadro si tu archivo tiene diferente orden\n",
    "vehiculos_color = _norm_cols(dfs[\"cuadro 19\"])\n",
    "\n",
    "# Detectar columna de color\n",
    "color_col = next((c for c in vehiculos_color.columns if \"color\" in c.lower()), None)\n",
    "if not color_col:\n",
    "    raise RuntimeError(\"No se encontró una columna 'Color' en el cuadro de vehículos por color. Verifica el número de cuadro.\")\n",
    "\n",
    "# Convertir todas las demás columnas numéricas y sumar\n",
    "num_cols = [c for c,t in vehiculos_color.dtypes if t in (\"int\",\"double\",\"float\",\"bigint\") and c != color_col]\n",
    "vehiculos_color = vehiculos_color.withColumnRenamed(color_col, \"color\")\n",
    "\n",
    "vehiculos_color_total = vehiculos_color.select(\n",
    "    \"color\",\n",
    "    sum([F.coalesce(F.col(c).cast(\"double\"), F.lit(0)) for c in num_cols]).alias(\"total_accidentes\")\n",
    ")\n",
    "\n",
    "# Agrupar por color por si hay filas repetidas\n",
    "vehiculos_color_total = vehiculos_color_total.groupBy(\"color\").agg(F.sum(\"total_accidentes\").alias(\"total_accidentes\"))\n",
    "\n",
    "# Ordenar descendente y limitar a 5\n",
    "top5_colores = vehiculos_color_total.orderBy(F.col(\"total_accidentes\").desc()).limit(5)\n",
    "\n",
    "display(top5_colores)  # en Databricks, usa gráfico de barras\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "8241d9e0-8fc8-49a0-80a4-689ec88e3287",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "3a8d050d-ff05-4f66-ba8a-6bb6326084ab",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**11**. (5 pts) Calcular cuántos lesionados por atropello hubo en 2024, por mes. Graficar en serie\n",
    "temporal (línea).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "284a85df-c5bc-4fd8-bf7d-6665fe775d75",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd, re\n",
    "from pyspark.sql import functions as F\n",
    "\n",
    "excel_path = \"/Volumes/workspace/default/lab8/Bases de datos principales PNC.xlsx\"\n",
    "\n",
    "# ----------------- utilidades -----------------\n",
    "MES_MAP = {\n",
    "    \"enero\":1,\"febrero\":2,\"marzo\":3,\"abril\":4,\"mayo\":5,\"junio\":6,\n",
    "    \"julio\":7,\"agosto\":8,\"septiembre\":9,\"setiembre\":9,\"octubre\":10,\"noviembre\":11,\"diciembre\":12,\n",
    "    \"ene\":1,\"feb\":2,\"mar\":3,\"abr\":4,\"may\":5,\"jun\":6,\"jul\":7,\"ago\":8,\"sep\":9,\"oct\":10,\"nov\":11,\"dic\":12,\n",
    "    \"1\":1,\"2\":2,\"3\":3,\"4\":4,\"5\":5,\"6\":6,\"7\":7,\"8\":8,\"9\":9,\"10\":10,\"11\":11,\"12\":12,\n",
    "}\n",
    "MES_KEYS = set(MES_MAP.keys())\n",
    "\n",
    "def s(x):\n",
    "    if not isinstance(x,str): x = \"\" if pd.isna(x) else str(x)\n",
    "    x = x.strip()\n",
    "    x = (x.replace(\"Á\",\"A\").replace(\"É\",\"E\").replace(\"Í\",\"I\").replace(\"Ó\",\"O\").replace(\"Ú\",\"U\")\n",
    "           .replace(\"á\",\"a\").replace(\"é\",\"e\").replace(\"í\",\"i\").replace(\"ó\",\"o\").replace(\"ú\",\"u\")\n",
    "           .replace(\"Ñ\",\"N\").replace(\"ñ\",\"n\"))\n",
    "    return x.lower()\n",
    "\n",
    "def is_month_token(tok): \n",
    "    return s(tok) in MES_KEYS\n",
    "\n",
    "def month_to_num(tok):\n",
    "    return MES_MAP.get(s(tok))\n",
    "\n",
    "def first_col_with(df, regex):\n",
    "    for c in df.columns:\n",
    "        col = df[c].astype(str).map(s)\n",
    "        if col.str.contains(regex, regex=True, na=False).any():\n",
    "            return c\n",
    "    return None\n",
    "\n",
    "# ----------------- escaneo robusto -----------------\n",
    "xls = pd.ExcelFile(excel_path)\n",
    "cuadros = [sh for sh in xls.sheet_names if sh.lower().startswith(\"cuadro\")]\n",
    "\n",
    "result = None\n",
    "used_sheet = None\n",
    "for sh in cuadros:\n",
    "    try:\n",
    "        raw = pd.read_excel(excel_path, sheet_name=sh, header=None, dtype=str)\n",
    "        if raw.empty or raw.shape[1] < 2: \n",
    "            continue\n",
    "\n",
    "        # --- Caso A: MESES COMO COLUMNAS (hay una fila con muchos nombres de mes) ---\n",
    "        header_row = None\n",
    "        for i in range(min(20, len(raw))):\n",
    "            row = [s(x) for x in raw.iloc[i].tolist()]\n",
    "            months_here = [c for c in row if is_month_token(c)]\n",
    "            if len(months_here) >= 6:\n",
    "                header_row = i\n",
    "                break\n",
    "        if header_row is not None:\n",
    "            pdf = pd.read_excel(excel_path, sheet_name=sh, header=header_row)\n",
    "            # normalizar encabezados simples\n",
    "            pdf.columns = [re.sub(r\"\\s+\",\"_\", s(c)) for c in pdf.columns]\n",
    "            first_col = pdf.columns[0]\n",
    "            # detectar fila \"atropello\"\n",
    "            mask_atr = pdf[first_col].astype(str).map(s).str.contains(r\"\\batropello\\b\", regex=True, na=False)\n",
    "            if mask_atr.any():\n",
    "                month_cols = [c for c in pdf.columns if is_month_token(c)]\n",
    "                if month_cols:\n",
    "                    longpdf = pdf.loc[mask_atr, [first_col]+month_cols].melt(\n",
    "                        id_vars=[first_col], var_name=\"mes\", value_name=\"lesionados\"\n",
    "                    )\n",
    "                    longpdf[\"mes_num\"] = longpdf[\"mes\"].map(month_to_num)\n",
    "                    longpdf[\"lesionados\"] = pd.to_numeric(longpdf[\"lesionados\"], errors=\"coerce\")\n",
    "                    longpdf = longpdf.dropna(subset=[\"mes_num\"])\n",
    "                    if not longpdf.empty:\n",
    "                        used_sheet = (sh, \"meses_en_columnas\")\n",
    "                        result = longpdf[[\"mes_num\",\"lesionados\"]].copy()\n",
    "                        break  # listo\n",
    "\n",
    "        # --- Caso B: MESES COMO FILAS (hay una columna con muchos nombres de mes) ---\n",
    "        # Busca una columna con >=6 tokens de mes\n",
    "        month_col = None\n",
    "        for c in raw.columns:\n",
    "            colvals = [s(v) for v in raw[c].tolist()[:200]]\n",
    "            if sum(1 for v in colvals if is_month_token(v)) >= 6:\n",
    "                month_col = c\n",
    "                break\n",
    "        if month_col is not None and result is None:\n",
    "            # Releer con header en la primera fila de datos no ayuda aquí; trabajamos \"as is\"\n",
    "            df = raw.copy()\n",
    "            df.columns = [f\"col_{i}\" for i in range(df.shape[1])]\n",
    "            # columna de meses:\n",
    "            meses_series = df[f\"col_{month_col}\"].astype(str).map(s)\n",
    "            # detectar la COLUMNA de \"Atropello\" en encabezados (una de las primeras ~10 filas tiene los headers)\n",
    "            # Heurística: toma la 1a fila no-nula como encabezado tentativo y también prueba hasta 10 filas\n",
    "            atrop_col_index = None\n",
    "            for hdr_row in range(min(10, len(df))):\n",
    "                headers = [s(x) for x in df.iloc[hdr_row].tolist()]\n",
    "                try:\n",
    "                    atrop_col_index = next((j for j,h in enumerate(headers) if re.search(r\"\\batropello\\b\", h)), None)\n",
    "                except Exception:\n",
    "                    atrop_col_index = None\n",
    "                if atrop_col_index is not None:\n",
    "                    # datos empiezan después de esa fila\n",
    "                    data = df.iloc[hdr_row+1:].copy()\n",
    "                    data.columns = headers  # aplicar encabezados\n",
    "                    # limpiar filas vacías\n",
    "                    data = data.dropna(how=\"all\", axis=0)\n",
    "                    # identificar columna de meses en los headers reales\n",
    "                    mes_header = next((h for h in data.columns if is_month_token(h)), None)\n",
    "                    if mes_header is None:\n",
    "                        # si no se reconoce por header, usa la serie original \"meses_series\"\n",
    "                        data[\"__mes\"] = meses_series.iloc[hdr_row+1:].values\n",
    "                        mes_header = \"__mes\"\n",
    "                    # tomar la columna de atropello\n",
    "                    atrop_col_name = data.columns[atrop_col_index] if atrop_col_index < len(data.columns) else None\n",
    "                    if atrop_col_name is None:\n",
    "                        continue\n",
    "                    sub = data[[mes_header, atrop_col_name]].rename(columns={mes_header:\"mes\", atrop_col_name:\"lesionados\"})\n",
    "                    sub[\"mes_num\"] = sub[\"mes\"].map(month_to_num)\n",
    "                    sub[\"lesionados\"] = pd.to_numeric(sub[\"lesionados\"], errors=\"coerce\")\n",
    "                    sub = sub.dropna(subset=[\"mes_num\"])\n",
    "                    if not sub.empty:\n",
    "                        used_sheet = (sh, \"meses_en_filas\", f\"header_row={hdr_row}\")\n",
    "                        result = sub[[\"mes_num\",\"lesionados\"]].copy()\n",
    "                        break\n",
    "            if result is not None:\n",
    "                break\n",
    "\n",
    "    except Exception:\n",
    "        continue\n",
    "\n",
    "if result is None or result.empty:\n",
    "    raise RuntimeError(\"No pude extraer la serie mensual de 'Atropello'. Abre visualmente 1–2 'cuadro X' donde veas Atropello y meses, y te adapto el lector exacto.\")\n",
    "\n",
    "# -------- a Spark y gráfico --------\n",
    "les_spark = spark.createDataFrame(result)\n",
    "lesionados_mes = (les_spark\n",
    "                  .groupBy(\"mes_num\")\n",
    "                  .agg(F.sum(F.col(\"lesionados\")).alias(\"total_lesionados\"))\n",
    "                  .orderBy(\"mes_num\"))\n",
    "\n",
    "print(\"Usando hoja:\", used_sheet)\n",
    "display(lesionados_mes)  # En el chart: Line • X=mes_num • Y=total_lesionados\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4eb32a3c-5fc5-4bbf-aa55-840f8084cf31",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F\n",
    "\n",
    "todos = spark.createDataFrame([(i,) for i in range(1,13)], [\"mes_num\"])\n",
    "lesionados_full = (todos\n",
    "    .join(lesionados_mes, on=\"mes_num\", how=\"left\")\n",
    "    .na.fill({\"total_lesionados\": 0})\n",
    "    .orderBy(\"mes_num\"))\n",
    "\n",
    "display(lesionados_full)  # luego elige Line chart: X=mes_num, Y=total_lesionados\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f170fdd4-b311-49c7-9ca6-a45c90ee9e76",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F\n",
    "\n",
    "# Partimos de lesionados_full con columnas: mes_num, total_lesionados\n",
    "\n",
    "# Construir pares planos (key, value, key, value, ...)\n",
    "pairs = []\n",
    "nombres = [\"Enero\",\"Febrero\",\"Marzo\",\"Abril\",\"Mayo\",\"Junio\",\n",
    "           \"Julio\",\"Agosto\",\"Septiembre\",\"Octubre\",\"Noviembre\",\"Diciembre\"]\n",
    "for i, nombre in enumerate(nombres, start=1):\n",
    "    pairs += [F.lit(str(i)), F.lit(nombre)]\n",
    "\n",
    "mes_nombre = F.create_map(*pairs)\n",
    "\n",
    "lesionados_labeled = lesionados_full.withColumn(\n",
    "    \"mes\",\n",
    "    mes_nombre[F.col(\"mes_num\").cast(\"string\")]\n",
    ")\n",
    "\n",
    "display(lesionados_labeled.select(\"mes_num\",\"mes\",\"total_lesionados\"))\n",
    "# En el gráfico, puedes usar X = mes_num (o mes) y Y = total_lesionados\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d7418e6c-24e8-4892-a3e0-420b998b373e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Convertir a Pandas para graficar fácilmente\n",
    "df_plot = lesionados_labeled.orderBy(\"mes_num\").toPandas()\n",
    "\n",
    "plt.figure(figsize=(8,5))\n",
    "plt.plot(df_plot[\"mes_num\"], df_plot[\"total_lesionados\"], marker=\"o\", linewidth=2)\n",
    "plt.title(\"Lesionados por atropello en 2024 (serie mensual)\")\n",
    "plt.xlabel(\"Mes\")\n",
    "plt.ylabel(\"Total de lesionados\")\n",
    "plt.xticks(df_plot[\"mes_num\"], df_plot[\"mes\"], rotation=45)\n",
    "plt.grid(True)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a9fa840b-fe9e-475d-8c2d-58ae0fd6c7f0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "12. Relacionar accidentes con fallecidos usando llaves (año, mes, departamento, tipo\n",
    "de accidente). Calcular el total de fallecidos por cada tipo de accidente. Graficar en barras\n",
    "horizontales"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b1a0bba1-6941-4349-9607-a72fecf47130",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# === Diagnóstico y extracción de FALLECIDOS (rápido y determinista) ===\n",
    "import pandas as pd, re\n",
    "from pyspark.sql import functions as F\n",
    "\n",
    "path = \"/Volumes/workspace/default/lab8/Bases de datos principales PNC.xlsx\"\n",
    "\n",
    "# --- utilidades de texto/mes ---\n",
    "MES = {\n",
    "    \"enero\":1,\"febrero\":2,\"marzo\":3,\"abril\":4,\"mayo\":5,\"junio\":6,\n",
    "    \"julio\":7,\"agosto\":8,\"septiembre\":9,\"setiembre\":9,\"octubre\":10,\"noviembre\":11,\"diciembre\":12,\n",
    "    \"ene\":1,\"feb\":2,\"mar\":3,\"abr\":4,\"may\":5,\"jun\":6,\"jul\":7,\"ago\":8,\"sep\":9,\"oct\":10,\"nov\":11,\"dic\":12,\n",
    "    \"1\":1,\"2\":2,\"3\":3,\"4\":4,\"5\":5,\"6\":6,\"7\":7,\"8\":8,\"9\":9,\"10\":10,\"11\":11,\"12\":12,\n",
    "}\n",
    "def norm(s):\n",
    "    if not isinstance(s,str): return \"\" if pd.isna(s) else str(s)\n",
    "    s = s.strip()\n",
    "    s = (s.replace(\"Á\",\"A\").replace(\"É\",\"E\").replace(\"Í\",\"I\").replace(\"Ó\",\"O\").replace(\"Ú\",\"U\")\n",
    "           .replace(\"á\",\"a\").replace(\"é\",\"e\").replace(\"í\",\"i\").replace(\"ó\",\"o\").replace(\"ú\",\"u\")\n",
    "           .replace(\"Ñ\",\"N\").replace(\"ñ\",\"n\")).lower()\n",
    "    return s\n",
    "def is_month(x): return norm(x) in MES\n",
    "def m2n(x): return MES.get(norm(x))\n",
    "\n",
    "# --- 1) buscar hojas candidatas con 'fallecid' y meses ---\n",
    "xls = pd.ExcelFile(path)\n",
    "sheets = [s for s in xls.sheet_names if s.lower().startswith(\"cuadro\")]\n",
    "candidates = []\n",
    "\n",
    "for sh in sheets:\n",
    "    try:\n",
    "        raw = pd.read_excel(path, sheet_name=sh, header=None, dtype=str)\n",
    "        if raw.empty: continue\n",
    "\n",
    "        # meses como columnas\n",
    "        header_row = None\n",
    "        for i in range(min(30, len(raw))):\n",
    "            row = [norm(x) for x in raw.iloc[i].tolist()]\n",
    "            if sum(1 for v in row if v in MES)>=6:\n",
    "                header_row = i; break\n",
    "\n",
    "        score = 0\n",
    "        has_fal_title = raw.head(8).astype(str).apply(lambda c: c.str.contains(\"fallecid\", case=False, na=False)).any().any()\n",
    "        if has_fal_title: score += 1\n",
    "        if header_row is not None: score += 1\n",
    "\n",
    "        # meses como filas\n",
    "        month_col = None\n",
    "        for c in raw.columns:\n",
    "            colvals = [norm(v) for v in raw[c].tolist()[:200]]\n",
    "            if sum(1 for v in colvals if v in MES)>=6:\n",
    "                month_col = c; break\n",
    "        if month_col is not None: score += 1\n",
    "\n",
    "        if score>=2:\n",
    "            candidates.append((sh, score, header_row, month_col))\n",
    "    except Exception:\n",
    "        pass\n",
    "\n",
    "print(\"Candidatos de 'Fallecidos' (sheet, score, header_row, month_col):\")\n",
    "for c in candidates[:10]:\n",
    "    print(\"  \", c)\n",
    "\n",
    "if not candidates:\n",
    "    raise RuntimeError(\"No encontré ninguna hoja con 'Fallecid*' y meses. Abre el Excel y dime qué 'cuadro X' lo contiene.\")\n",
    "\n",
    "# --- 2) usar el mejor candidato y extraer a largo ---\n",
    "best = candidates[0][0]   # tomamos el primero (mayor score)\n",
    "raw = pd.read_excel(path, sheet_name=best, header=None, dtype=str)\n",
    "\n",
    "# detectar encabezado si meses son columnas\n",
    "header_row = None\n",
    "for i in range(min(30, len(raw))):\n",
    "    row = [norm(x) for x in raw.iloc[i].tolist()]\n",
    "    if sum(1 for v in row if v in MES)>=6: header_row = i; break\n",
    "\n",
    "if header_row is not None:\n",
    "    pdf = pd.read_excel(path, sheet_name=best, header=header_row)\n",
    "    pdf.columns = [norm(c).replace(\" \",\"_\") for c in pdf.columns]\n",
    "    first = pdf.columns[0]\n",
    "    month_cols = [c for c in pdf.columns if norm(c) in MES]\n",
    "    if not month_cols:\n",
    "        raise RuntimeError(f\"{best}: no identifiqué columnas de meses.\")\n",
    "    # ¿hay columna explícita de fallecidos? si no, asumimos que cada mes es 'fallecidos'\n",
    "    id_vars = [first] + [c for c in pdf.columns if c not in month_cols and c not in [first]]\n",
    "    long = pdf.melt(id_vars=id_vars, value_vars=month_cols, var_name=\"mes\", value_name=\"valor\")\n",
    "    long[\"mes_num\"] = long[\"mes\"].map(m2n)\n",
    "    long[\"tipo_accidente\"] = pdf[first]\n",
    "    # intenta detectar 'fallecid' en nombres\n",
    "    if not any(\"fallecid\" in col for col in pdf.columns):\n",
    "        long.rename(columns={\"valor\":\"fallecidos\"}, inplace=True)\n",
    "    else:\n",
    "        # si existiera columna específica, úsala\n",
    "        falcol = [c for c in pdf.columns if \"fallecid\" in c]\n",
    "        if falcol:\n",
    "            long[\"fallecidos\"] = pd.to_numeric(pdf[falcol[0]], errors=\"coerce\")\n",
    "        else:\n",
    "            long[\"fallecidos\"] = pd.to_numeric(long[\"valor\"], errors=\"coerce\")\n",
    "else:\n",
    "    # meses como filas: encontrar columna de meses y columna con fallecidos\n",
    "    # asumimos que en alguna de las primeras 10 filas están los headers\n",
    "    hdr = None\n",
    "    for r in range(min(10,len(raw))):\n",
    "        head = [norm(x) for x in raw.iloc[r].tolist()]\n",
    "        if any(\"fallecid\" in h for h in head):\n",
    "            hdr = r; break\n",
    "    if hdr is None:\n",
    "        raise RuntimeError(f\"{best}: no encontré fila de encabezado con 'Fallecid*'.\")\n",
    "    pdf = pd.read_excel(path, sheet_name=best, header=hdr, dtype=str)\n",
    "    pdf.columns = [norm(c).replace(\" \",\"_\") for c in pdf.columns]\n",
    "    mcol = None\n",
    "    for c in pdf.columns:\n",
    "        if pdf[c].astype(str).map(is_month).sum()>=6:\n",
    "            mcol = c; break\n",
    "    falcol = next((c for c in pdf.columns if \"fallecid\" in c), None)\n",
    "    tcol  = next((c for c in pdf.columns if \"tipo\" in c), None)\n",
    "    if mcol is None or falcol is None:\n",
    "        raise RuntimeError(f\"{best}: identifiqué meses/fallecidos incompletos (mes={mcol}, fallecidos={falcol}).\")\n",
    "    long = pd.DataFrame({\n",
    "        \"mes_num\": pdf[mcol].map(m2n),\n",
    "        \"fallecidos\": pd.to_numeric(pdf[falcol], errors=\"coerce\"),\n",
    "        \"tipo_accidente\": pdf[tcol] if tcol in pdf.columns else \"na\"\n",
    "    })\n",
    "\n",
    "# limpiar y suponer 2024 si no hay año explícito\n",
    "long = long.dropna(subset=[\"mes_num\"])\n",
    "long[\"anio\"] = 2024\n",
    "long[\"tipo_accidente\"] = long[\"tipo_accidente\"].astype(str).str.strip().str.lower()\n",
    "\n",
    "# a Spark y agregar por tipo (inciso 12 pide barras por tipo)\n",
    "fal_s_simple = spark.createDataFrame(long[[\"anio\",\"mes_num\",\"tipo_accidente\",\"fallecidos\"]])\n",
    "res_12_simple = (fal_s_simple\n",
    "    .filter(F.col(\"anio\")==2024)\n",
    "    .groupBy(\"tipo_accidente\")\n",
    "    .agg(F.sum(\"fallecidos\").alias(\"total_fallecidos\"))\n",
    "    .orderBy(F.desc(\"total_fallecidos\")))\n",
    "\n",
    "print(\"Usando hoja de fallecidos:\", best)\n",
    "display(res_12_simple)  # luego elige: Bar -> Values(X)=total_fallecidos, Keys(Y)=tipo_accidente, Orientation=Horizontal\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3b1ae6de-b784-49c1-8de4-981c91f2ce43",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "# Crear DataFrame con tus resultados\n",
    "data = {\n",
    "    \"tipo_accidente\": [\n",
    "        \"guatemala\", \"escuintla\", \"alta verapaz\", \"sacatepéquez\", \"chimaltenango\",\n",
    "        \"petén\", \"jutiapa\", \"baja verapaz\", \"san marcos\", \"izabal\", \"santa rosa\",\n",
    "        \"suchitepéquez\", \"el progreso\", \"quetzaltenango\", \"retalhuleu\", \"jalapa\",\n",
    "        \"quiché\", \"sololá\", \"chiquimula\", \"huehuetenango\", \"zacapa\", \"totonicapán\"\n",
    "    ],\n",
    "    \"total_fallecidos\": [\n",
    "        263, 81, 29, 27, 25, 24, 24, 18, 16, 16, 15, 13, 12, 11, 11, 11, 7, 6, 6, 5, 4, 2\n",
    "    ]\n",
    "}\n",
    "\n",
    "df = pd.DataFrame(data).sort_values(\"total_fallecidos\", ascending=True)\n",
    "\n",
    "# === Gráfica ===\n",
    "plt.figure(figsize=(9, 6))\n",
    "bars = plt.barh(df[\"tipo_accidente\"], df[\"total_fallecidos\"], color=\"#1f77b4\")\n",
    "plt.title(\"Total de fallecidos por tipo de accidente (2024)\", fontsize=13)\n",
    "plt.xlabel(\"Total de fallecidos\")\n",
    "plt.ylabel(\"Tipo de accidente\")\n",
    "plt.grid(axis=\"x\", linestyle=\"--\", alpha=0.6)\n",
    "\n",
    "# Etiquetas al final de cada barra\n",
    "for bar in bars:\n",
    "    width = bar.get_width()\n",
    "    plt.text(width + 2, bar.get_y() + bar.get_height()/2,\n",
    "             f\"{int(width)}\", va='center', fontsize=9)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "063c3621-2344-4583-beff-efb1109c63b7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "> 13. Usar withColumn para clasificar accidentes en franjas horarias: Mañana [6-12), Tarde [12-18), Noche [18-24), Madrugada [0-6). Mostrar cuántos accidentes ocurren en cada franja"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c6c1d6a9-7234-400d-a015-e701d7a12340",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# === Inciso 13: detección automática de horas + clasificación por franja ===\n",
    "import re\n",
    "import pandas as pd\n",
    "from pyspark.sql import functions as F\n",
    "\n",
    "excel_path = \"/Volumes/workspace/default/lab8/Bases de datos principales PNC.xlsx\"\n",
    "\n",
    "def norm(s):\n",
    "    if not isinstance(s, str):\n",
    "        s = \"\" if pd.isna(s) else str(s)\n",
    "    s = s.strip()\n",
    "    s = (s.replace(\"Á\",\"A\").replace(\"É\",\"E\").replace(\"Í\",\"I\").replace(\"Ó\",\"O\").replace(\"Ú\",\"U\")\n",
    "           .replace(\"á\",\"a\").replace(\"é\",\"e\").replace(\"í\",\"i\").replace(\"ó\",\"o\").replace(\"ú\",\"u\")\n",
    "           .replace(\"Ñ\",\"N\").replace(\"ñ\",\"n\")).lower()\n",
    "    return s\n",
    "\n",
    "# ¿Luce como hora (0–23, “06”, “6”, “06:00”, “6-7”, “6–7”, “06 a 07”, etc.)?\n",
    "H_RE = re.compile(r\"^(\\d{1,2})(?::\\d{2})?$\")         # 6, 06, 06:00\n",
    "RNG_RE = re.compile(r\"^(\\d{1,2})\\s*[-–a]\\s*(\\d{1,2})\")# 6-7, 6–7, 6 a 7\n",
    "\n",
    "def token_to_hour(tok):\n",
    "    t = norm(tok)\n",
    "    m = H_RE.match(t)\n",
    "    if m:\n",
    "        h = int(m.group(1))\n",
    "        return h if 0 <= h <= 23 else None\n",
    "    m2 = RNG_RE.match(t)\n",
    "    if m2:\n",
    "        h = int(m2.group(1))\n",
    "        return h if 0 <= h <= 23 else None\n",
    "    return None\n",
    "\n",
    "def looks_like_hour_row(vals):\n",
    "    vals = [v for v in vals if str(v).strip() != \"\"]\n",
    "    hits = sum(1 for v in vals if token_to_hour(v) is not None)\n",
    "    return hits >= 6  # una fila con muchas \"horas\" como columnas\n",
    "\n",
    "xls = pd.ExcelFile(excel_path)\n",
    "sheets = [s for s in xls.sheet_names if s.lower().startswith(\"cuadro\")]\n",
    "\n",
    "hours_long_pd = None\n",
    "used_sheet = None\n",
    "\n",
    "for sh in sheets:\n",
    "    try:\n",
    "        raw = pd.read_excel(excel_path, sheet_name=sh, header=None, dtype=str)\n",
    "        if raw.empty: \n",
    "            continue\n",
    "\n",
    "        # --- Caso A: horas como columnas (buscar una fila encabezado con muchas horas)\n",
    "        header_row = None\n",
    "        for i in range(min(40, len(raw))):\n",
    "            if looks_like_hour_row(raw.iloc[i].tolist()):\n",
    "                header_row = i\n",
    "                break\n",
    "\n",
    "        if header_row is not None:\n",
    "            pdf = pd.read_excel(excel_path, sheet_name=sh, header=header_row)\n",
    "            pdf.columns = [norm(c).replace(\" \", \"_\") for c in pdf.columns]\n",
    "            first = pdf.columns[0]\n",
    "            hour_cols = [c for c in pdf.columns if token_to_hour(c) is not None]\n",
    "            if hour_cols:\n",
    "                # melt a largo\n",
    "                long = pdf.melt(id_vars=[first], value_vars=hour_cols,\n",
    "                                var_name=\"hora_tok\", value_name=\"accidentes\")\n",
    "                long[\"hora_num\"] = long[\"hora_tok\"].map(token_to_hour)\n",
    "                long[\"accidentes\"] = pd.to_numeric(long[\"accidentes\"], errors=\"coerce\")\n",
    "                long = long.dropna(subset=[\"hora_num\"])\n",
    "                if not long.empty:\n",
    "                    hours_long_pd = long[[\"hora_num\",\"accidentes\"]]\n",
    "                    used_sheet = (sh, \"horas_en_columnas\", f\"header_row={header_row}\")\n",
    "                    break\n",
    "\n",
    "        # --- Caso B: horas como filas (buscar una columna con muchas horas)\n",
    "        hour_col_idx = None\n",
    "        for c in raw.columns:\n",
    "            colvals = raw[c].tolist()[:200]\n",
    "            hits = sum(1 for v in colvals if token_to_hour(v) is not None)\n",
    "            if hits >= 6:\n",
    "                hour_col_idx = c\n",
    "                break\n",
    "\n",
    "        if hours_long_pd is None and hour_col_idx is not None:\n",
    "            # Adivina encabezado: alguna fila temprana que contenga \"total\" o algo numérico\n",
    "            header_guess = None\n",
    "            for r in range(min(10, len(raw))):\n",
    "                vals = [norm(x) for x in raw.iloc[r].tolist()]\n",
    "                if any(\"total\" in v for v in vals) or any(re.search(r\"\\d\", v) for v in vals):\n",
    "                    header_guess = r\n",
    "                    break\n",
    "            if header_guess is None:\n",
    "                header_guess = 0\n",
    "\n",
    "            pdf = pd.read_excel(excel_path, sheet_name=sh, header=header_guess, dtype=str)\n",
    "            pdf.columns = [norm(c).replace(\" \", \"_\") for c in pdf.columns]\n",
    "\n",
    "            # localizar col con hora y col con accidentes (o total)\n",
    "            hora_col = None\n",
    "            for c in pdf.columns:\n",
    "                if pdf[c].astype(str).map(lambda x: token_to_hour(x) is not None).sum() >= 6:\n",
    "                    hora_col = c; break\n",
    "\n",
    "            # heurística de métrica: buscar “acciden”, “hecho”, “event”, “total”\n",
    "            val_col = next((c for c in pdf.columns if re.search(r\"(acciden|hech|event|total)\", c)), None)\n",
    "            if hora_col and val_col:\n",
    "                tmp = pd.DataFrame({\n",
    "                    \"hora_num\": pdf[hora_col].map(token_to_hour),\n",
    "                    \"accidentes\": pd.to_numeric(pdf[val_col], errors=\"coerce\")\n",
    "                }).dropna(subset=[\"hora_num\"])\n",
    "                if not tmp.empty:\n",
    "                    hours_long_pd = tmp[[\"hora_num\",\"accidentes\"]]\n",
    "                    used_sheet = (sh, \"horas_en_filas\", f\"header_row={header_guess}\")\n",
    "                    break\n",
    "    except Exception:\n",
    "        continue\n",
    "\n",
    "if hours_long_pd is None or hours_long_pd.empty:\n",
    "    raise RuntimeError(\"No pude localizar horas en los 'cuadro X'. Abre el Excel y dime cuál cuadro tiene los accidentes por hora para ajustarlo exacto.\")\n",
    "\n",
    "print(\"Usando hoja para horas:\", used_sheet)\n",
    "\n",
    "# --- A Spark ---\n",
    "df_horas = spark.createDataFrame(hours_long_pd)\n",
    "\n",
    "# Clasificación por franja con withColumn\n",
    "df_franjas = (df_horas\n",
    "    .withColumn(\n",
    "        \"franja_horaria\",\n",
    "        F.when((F.col(\"hora_num\") >= 6) & (F.col(\"hora_num\") < 12), \"Mañana\")\n",
    "         .when((F.col(\"hora_num\") >= 12) & (F.col(\"hora_num\") < 18), \"Tarde\")\n",
    "         .when((F.col(\"hora_num\") >= 18) & (F.col(\"hora_num\") < 24), \"Noche\")\n",
    "         .otherwise(\"Madrugada\")\n",
    "    )\n",
    ")\n",
    "\n",
    "res_13 = (df_franjas\n",
    "    .groupBy(\"franja_horaria\")\n",
    "    .agg(F.sum(F.col(\"accidentes\")).alias(\"total_accidentes\"))\n",
    "    .orderBy(F.expr(\"CASE franja_horaria WHEN 'Madrugada' THEN 0 WHEN 'Mañana' THEN 1 WHEN 'Tarde' THEN 2 WHEN 'Noche' THEN 3 END\"))\n",
    ")\n",
    "\n",
    "display(res_13) \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 14. Calcular el ratio de fallecidos por accidente en cada departamento (fallecidos / accidentes). Guardar el resultado en Parquet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F\n",
    "import re\n",
    "\n",
    "def _norm_cols(df):\n",
    "    try:\n",
    "        return normalizar_cols(df)\n",
    "    except NameError:\n",
    "        d = df\n",
    "        for c in d.columns:\n",
    "            d = d.withColumnRenamed(c, c.strip().lower().replace(\" \", \"_\"))\n",
    "        return d\n",
    "\n",
    "def _find_depto(df):\n",
    "    for c in df.columns:\n",
    "        if re.search(r\"depto|depart\", c, re.I):\n",
    "            return c\n",
    "    return df.columns[0]\n",
    "\n",
    "def _year_col(df, year=\"2024\"):\n",
    "    for c in df.columns:\n",
    "        if re.fullmatch(rf\"\\s*{year}\\s*\", str(c)):\n",
    "            return c\n",
    "    for c in df.columns:\n",
    "        if re.match(r\"^\\s*\\d{4}\", str(c)) and str(c).strip().startswith(year):\n",
    "            return c\n",
    "    raise RuntimeError(f\"No encontré la columna de año {year} en el cuadro.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c1 = _norm_cols(dfs[\"cuadro 1\"])\n",
    "dept1 = _find_depto(c1)\n",
    "ycol1 = _year_col(c1, \"2024\")\n",
    "\n",
    "acc = (c1.select(F.col(dept1).alias(\"departamento\"),\n",
    "                 F.col(ycol1).cast(\"double\").alias(\"accidentes\"))\n",
    "         .where(~F.lower(F.col(\"departamento\")).like(\"total%\"))\n",
    "         .withColumn(\"departamento_key\", F.lower(F.trim(F.col(\"departamento\")))))\n",
    "\n",
    "# Fallecidos 2024 por departamento cuadro 47\n",
    "c47 = _norm_cols(dfs[\"cuadro 47\"])\n",
    "dept47 = _find_depto(c47)\n",
    "ycol47 = _year_col(c47, \"2024\")\n",
    "\n",
    "fal = (c47.select(F.col(dept47).alias(\"departamento\"),\n",
    "                  F.col(ycol47).cast(\"double\").alias(\"fallecidos\"))\n",
    "         .where(~F.lower(F.col(\"departamento\")).like(\"total%\"))\n",
    "         .withColumn(\"departamento_key\", F.lower(F.trim(F.col(\"departamento\")))))\n",
    "\n",
    "# Ratio fallecidos/accidentes\n",
    "ratio = (fal.join(acc.select(\"departamento_key\",\"accidentes\"),\n",
    "                  on=\"departamento_key\", how=\"inner\")\n",
    "           .select(\n",
    "               F.col(\"departamento_key\").alias(\"departamento\"),\n",
    "               F.col(\"fallecidos\"),\n",
    "               F.col(\"accidentes\"),\n",
    "               (F.col(\"fallecidos\") /\n",
    "                F.when(F.col(\"accidentes\") > 0, F.col(\"accidentes\")).otherwise(F.lit(None))\n",
    "               ).alias(\"ratio_fallecidos_por_accidente\")\n",
    "           )\n",
    "           .orderBy(F.desc(\"ratio_fallecidos_por_accidente\")))\n",
    "\n",
    "\n",
    "out_dir = OUT_DIR if 'OUT_DIR' in locals() else f\"{BASE_DIR}/output\"\n",
    "dbutils.fs.mkdirs(out_dir)\n",
    "out_path = f\"{out_dir}/ratio_fallecidos_por_accidente_2024.parquet\"\n",
    "ratio.write.mode(\"overwrite\").parquet(out_path)\n",
    "\n",
    "ratio_reload = spark.read.parquet(out_path)\n",
    "print(\"Guardado en:\", out_path)\n",
    "display(ratio_reload.orderBy(F.desc(\"ratio_fallecidos_por_accidente\")))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 15. dentificar los grupos de edad más afectados en fallecidos y lesionados. Graficar en barras que permitan comparar a ambos grupos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _norm_cols(df):\n",
    "    try: return normalizar_cols(df)\n",
    "    except NameError:\n",
    "        d=df\n",
    "        for c in d.columns: d=d.withColumnRenamed(c, c.strip().lower().replace(\" \",\"_\"))\n",
    "        return d\n",
    "\n",
    "def _numeric_cols(df):\n",
    "    prim=(\"byte\",\"short\",\"int\",\"bigint\",\"long\",\"float\",\"double\")\n",
    "    return [c for c,t in df.dtypes if t in prim or t.lower().startswith(\"decimal\")]\n",
    "\n",
    "def _sum_by_age_cols(df):\n",
    "    df = _norm_cols(df)\n",
    "    dept = next((c for c in df.columns if re.search(r\"depto|depart\", c, re.I)), None)\n",
    "    if dept: df = df.where(~F.lower(F.col(dept)).like(\"total%\"))\n",
    "    cols = [c for c in _numeric_cols(df) if c.lower() != \"total\"]\n",
    "    long = (df.select(F.explode(\n",
    "                F.arrays_zip(F.array(*[F.lit(c) for c in cols]),\n",
    "                             F.array(*[F.col(c).cast(\"double\") for c in cols]))\n",
    "           ).alias(\"kv\"))\n",
    "           .select(F.col(\"kv.0\").alias(\"grupo\"), F.col(\"kv.1\").alias(\"valor\"))\n",
    "           .groupBy(\"grupo\").agg(F.sum(\"valor\").alias(\"total\")))\n",
    "    return long\n",
    "\n",
    "les = _sum_by_age_cols(dfs[\"cuadro 34\"]).withColumnRenamed(\"total\",\"lesionados\")\n",
    "fal = _sum_by_age_cols(dfs[\"cuadro 50\"]).withColumnRenamed(\"total\",\"fallecidos\")\n",
    "\n",
    "res15 = (fal.join(les, on=\"grupo\", how=\"outer\")\n",
    "           .na.fill(0)\n",
    "           .withColumn(\"grupo\", F.regexp_replace(F.lower(\"grupo\"), \"_\", \" \"))\n",
    "           .withColumn(\"total\", F.col(\"fallecidos\")+F.col(\"lesionados\"))\n",
    "           .orderBy(F.desc(\"total\")))\n",
    "\n",
    "display(res15.select(\"grupo\",\"fallecidos\",\"lesionados\"))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 16. Calcular, para el municipio de Guatemala, cuántos accidentes hay por zona y cuántos fallecidos se reportan en cada una. Generar un gráfico de barras con ambos indicadores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prim = (\"byte\",\"short\",\"int\",\"bigint\",\"long\",\"float\",\"double\")\n",
    "\n",
    "def _norm_cols(df):\n",
    "    try: return normalizar_cols(df)\n",
    "    except NameError:\n",
    "        d=df\n",
    "        for c in d.columns: d=d.withColumnRenamed(c, c.strip().lower().replace(\" \",\"_\"))\n",
    "        return d\n",
    "\n",
    "def _sum_wide_by_zone(df, out_col):\n",
    "    d = _norm_cols(df)\n",
    "    num = [c for c,t in d.dtypes if (t in prim or t.lower().startswith(\"decimal\")) and c.lower()!=\"total\"]\n",
    "    long = (d.select(F.explode(\n",
    "                F.arrays_zip(F.array(*[F.lit(c) for c in num]),\n",
    "                             F.array(*[F.col(c).cast(\"double\") for c in num]))\n",
    "            ).alias(\"kv\"))\n",
    "            .select(F.lower(F.trim(F.col(\"kv.0\"))).alias(\"zona\"),\n",
    "                    F.col(\"kv.1\").alias(out_col))\n",
    "            .groupBy(\"zona\").agg(F.sum(out_col).alias(out_col)))\n",
    "    return long\n",
    "\n",
    "acc_src = next(s for s in [\"cuadro 14\",\"cuadro 15\",\"cuadro 16\"] if s in dfs)\n",
    "fal_src = next(s for s in [\"cuadro 61\",\"cuadro 60\",\"cuadro 62\"] if s in dfs)\n",
    "\n",
    "acc_zona = _sum_wide_by_zone(dfs[acc_src], \"accidentes\")\n",
    "fal_zona = _sum_wide_by_zone(dfs[fal_src], \"fallecidos\")\n",
    "\n",
    "res16 = (acc_zona.join(fal_zona, on=\"zona\", how=\"outer\")\n",
    "         .na.fill(0)\n",
    "         .withColumn(\"zona\", F.regexp_replace(\"zona\", \"_\", \" \"))\n",
    "         .withColumn(\"total\", F.col(\"accidentes\")+F.col(\"fallecidos\"))\n",
    "         .orderBy(F.desc(\"total\")))\n",
    "\n",
    "display(res16.select(\"zona\",\"accidentes\",\"fallecidos\"))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 17. Crear un DataFrame que muestre el porcentaje de accidentes donde el conductor era hombre vs mujer (tabla vehículos). Guardar como Parquet. Finalmente, vuélvalo a cargar y grafique con display en gráfico de pie."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _norm_cols(df):\n",
    "    try: return normalizar_cols(df)\n",
    "    except NameError:\n",
    "        d=df\n",
    "        for c in d.columns: d=d.withColumnRenamed(c, c.strip().lower().replace(\" \",\"_\"))\n",
    "        return d\n",
    "\n",
    "src = next((c for c in [\"cuadro 22\",\"cuadro 23\",\"cuadro 24\",\"cuadro 25\",\"cuadro 27\"] if c in dfs), None)\n",
    "if src is None:\n",
    "    raise RuntimeError(\"No encontré cuadros de vehículos por sexo/condición del conductor (22–25,27).\")\n",
    "d = _norm_cols(dfs[src])\n",
    "\n",
    "maybe_dim = next((c for c in d.columns if re.search(r\"depto|depart|zona|mes|dia|hora|tipo\", c, re.I)), None)\n",
    "if maybe_dim:\n",
    "    d = d.where(~F.lower(F.col(maybe_dim).cast(\"string\")).like(\"total%\"))\n",
    "\n",
    "male_pat, female_pat = re.compile(r\"(hombre|masculin)\", re.I), re.compile(r\"(mujer|femenin)\", re.I)\n",
    "num_prims = (\"byte\",\"short\",\"int\",\"bigint\",\"long\",\"float\",\"double\",\"decimal\")\n",
    "num_cols = [c for c,t in d.dtypes if (t in num_prims or str(t).lower().startswith(\"decimal\")) and c.lower()!=\"total\"]\n",
    "\n",
    "male_cols   = [c for c in num_cols if male_pat.search(c)]\n",
    "female_cols = [c for c in num_cols if female_pat.search(c)]\n",
    "\n",
    "if male_cols and female_cols:\n",
    "    # Formato ancho: sumar columnas por sexo\n",
    "    male_row   = d.agg(*[F.sum(F.coalesce(F.col(c).cast(\"double\"), F.lit(0))).alias(c) for c in male_cols]).first()\n",
    "    female_row = d.agg(*[F.sum(F.coalesce(F.col(c).cast(\"double\"), F.lit(0))).alias(c) for c in female_cols]).first()\n",
    "    male_total   = float(sum((male_row[c] or 0) for c in male_cols))\n",
    "    female_total = float(sum((female_row[c] or 0) for c in female_cols))\n",
    "else:\n",
    "    # Formato largo: agrupar por columna 'sexo'\n",
    "    sexo_col = next((c for c in d.columns if re.search(r\"\\bsexo\\b\", c, re.I)), None)\n",
    "    if not sexo_col:\n",
    "        raise RuntimeError(f\"{src}: no hallé columnas 'hombre/mujer' ni columna 'sexo'.\")\n",
    "    long = (d.select(F.lower(F.trim(F.col(sexo_col))).alias(\"sexo\"), *[F.col(c).cast(\"double\") for c in num_cols])\n",
    "              .groupBy(\"sexo\")\n",
    "              .agg(sum(F.coalesce(F.col(c),F.lit(0.0)) for c in num_cols).alias(\"valor\")))\n",
    "    male_total   = float(long.where(F.col(\"sexo\").rlike(\"hombre|masculin\")).agg(F.sum(\"valor\")).first()[0] or 0)\n",
    "    female_total = float(long.where(F.col(\"sexo\").rlike(\"mujer|femenin\")).agg(F.sum(\"valor\")).first()[0] or 0)\n",
    "\n",
    "total = male_total + female_total if (male_total + female_total) else 1.0\n",
    "df17 = spark.createDataFrame(\n",
    "    [(\"Hombre\", male_total), (\"Mujer\", female_total)],\n",
    "    [\"sexo_conductor\",\"conteo\"]\n",
    ").withColumn(\"porcentaje\", F.round(F.col(\"conteo\")/F.lit(total)*100, 2))\n",
    "\n",
    "out_dir = OUT_DIR if 'OUT_DIR' in locals() else f\"{BASE_DIR}/output\"\n",
    "dbutils.fs.mkdirs(out_dir)\n",
    "out_path = f\"{out_dir}/porcentaje_accidentes_por_sexo_conductor_2024.parquet\"\n",
    "df17.write.mode(\"overwrite\").parquet(out_path)\n",
    "df17_reload = spark.read.parquet(out_path).orderBy(F.desc(\"porcentaje\"))\n",
    "\n",
    "print(\"Guardado en:\", out_path)\n",
    "display(df17_reload)  "
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "3"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "Lab 8",
   "widgets": {}
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
